{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purely for google colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \"/content/drive/MyDrive/fypj_final_submission\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appdirs==1.4.4\n",
      "argon2-cffi==21.3.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "asttokens==2.0.5\n",
      "attrs==21.4.0\n",
      "audiomentations==0.24.0\n",
      "audioread==2.1.9\n",
      "auto-sklearn==0.14.6\n",
      "backcall==0.2.0\n",
      "beautifulsoup4==4.11.1\n",
      "bleach==5.0.0\n",
      "certifi==2021.10.8\n",
      "cffi==1.15.0\n",
      "charset-normalizer==2.0.12\n",
      "click==8.1.3\n",
      "cloudpickle==2.0.0\n",
      "ConfigSpace==0.4.14\n",
      "cycler==0.11.0\n",
      "Cython==0.29.28\n",
      "dask==2022.5.0\n",
      "debugpy==1.6.0\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "distributed==2022.5.0\n",
      "distro==1.7.0\n",
      "emcee==3.1.1\n",
      "entrypoints==0.4\n",
      "et-xmlfile==1.1.0\n",
      "executing==0.8.3\n",
      "fastjsonschema==2.15.3\n",
      "fonttools==4.33.3\n",
      "fsspec==2022.3.0\n",
      "future==0.18.2\n",
      "HeapDict==1.0.1\n",
      "idna==3.3\n",
      "importlib-resources==5.7.1\n",
      "ipykernel==6.13.0\n",
      "ipython==8.3.0\n",
      "ipython-genutils==0.2.0\n",
      "jedi==0.18.1\n",
      "Jinja2==3.1.2\n",
      "joblib==1.1.0\n",
      "jsonschema==4.4.0\n",
      "jupyter-client==7.3.0\n",
      "jupyter-core==4.10.0\n",
      "jupyterlab-pygments==0.2.2\n",
      "kiwisolver==1.4.2\n",
      "liac-arff==2.5.0\n",
      "librosa==0.9.1\n",
      "llvmlite==0.36.0\n",
      "locket==1.0.0\n",
      "MarkupSafe==2.1.1\n",
      "matplotlib==3.5.2\n",
      "matplotlib-inline==0.1.3\n",
      "mistune==0.8.4\n",
      "msgpack==1.0.3\n",
      "nbclient==0.6.2\n",
      "nbconvert==6.5.0\n",
      "nbformat==5.4.0\n",
      "nest-asyncio==1.5.5\n",
      "networkx==2.8\n",
      "notebook==6.4.11\n",
      "numba==0.53.0\n",
      "numpy==1.21.0\n",
      "openpyxl==3.0.9\n",
      "packaging==21.3\n",
      "pandas==1.4.2\n",
      "pandocfilters==1.5.0\n",
      "parso==0.8.3\n",
      "partd==1.2.0\n",
      "pexpect==4.8.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==9.1.0\n",
      "pipelineprofiler==0.1.17\n",
      "pooch==1.6.0\n",
      "prometheus-client==0.14.1\n",
      "prompt-toolkit==3.0.29\n",
      "psutil==5.9.0\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "pycparser==2.21\n",
      "pydub==0.25.1\n",
      "Pygments==2.12.0\n",
      "pyloudnorm==0.1.0\n",
      "pynisher==0.6.4\n",
      "pyparsing==3.0.8\n",
      "pyrfr==0.8.2\n",
      "pyrsistent==0.18.1\n",
      "python-dateutil==2.8.2\n",
      "pytz==2022.1\n",
      "PyWavelets==1.3.0\n",
      "PyYAML==6.0\n",
      "pyzmq==22.3.0\n",
      "requests==2.27.1\n",
      "resampy==0.2.2\n",
      "scikit-learn==0.24.2\n",
      "scipy==1.8.0\n",
      "Send2Trash==1.8.0\n",
      "six==1.16.0\n",
      "smac==1.2\n",
      "sortedcontainers==2.4.0\n",
      "SoundFile==0.10.3.post1\n",
      "soupsieve==2.3.2.post1\n",
      "stack-data==0.2.0\n",
      "tblib==1.7.0\n",
      "terminado==0.13.3\n",
      "threadpoolctl==3.1.0\n",
      "tinycss2==1.1.1\n",
      "toolz==0.11.2\n",
      "tornado==6.1\n",
      "tqdm==4.64.0\n",
      "traitlets==5.1.1\n",
      "typing-extensions==4.2.0\n",
      "urllib3==1.26.9\n",
      "wcwidth==0.2.5\n",
      "webencodings==0.5.1\n",
      "zict==2.2.0\n",
      "zipp==3.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pipelineprofiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install auto-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa \n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from audiomentations import *\n",
    "import warnings\n",
    "import scipy.signal\n",
    "from pydub import AudioSegment\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augments\n",
    "augment = Compose([\n",
    "    Reverse(p=0.8),\n",
    "    PolarityInversion(p=1)\n",
    "])\n",
    "\n",
    "augment_spec = SpecCompose([\n",
    "    SpecFrequencyMask(p=1)\n",
    "])\n",
    "\n",
    "def augment_signal(S,sr,outputpath):\n",
    "    augmented_signal = augment(S,sr)\n",
    "    sf.write(outputpath,augmented_signal,sr)\n",
    "\n",
    "\n",
    "# for balancing\n",
    "def grab_id(amt,val_srs):\n",
    "    temp_idx=[]\n",
    "    temp_count=0\n",
    "    for i,k in val_srs.iteritems():\n",
    "        if temp_count >= amt:\n",
    "            break\n",
    "        else:\n",
    "            temp_count+=k\n",
    "            temp_idx.append(i)\n",
    "    return temp_idx\n",
    "\n",
    "# feature extraction functions\n",
    "def mfccs_features_extract(S,sr):\n",
    "    mfccs_features = librosa.feature.mfcc(y=S, sr=sr, n_mfcc= 40)\n",
    "    mfccs_features = librosa.decompose.nn_filter(mfccs_features)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T, axis= 0)\n",
    "    return mfccs_scaled_features,mfccs_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_path = \"./1.0.1/\"\n",
    "augment_path = \"./1.0.1/training_data_balanced/\"\n",
    "records = data_path+ \"RECORDS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution by patient id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Absent     0.737792\n",
       "Present    0.190021\n",
       "Unknown    0.072187\n",
       "Name: Murmur, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics = pd.read_csv(data_path+\"training_data.csv\")\n",
    "print(\"Class distribution by patient id\")\n",
    "demographics[\"Murmur\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution by audio recordings\n",
      " Absent     2391\n",
      "Present     616\n",
      "Unknown     156\n",
      "Name: Murmur, dtype: int64\n",
      "\n",
      "Training set class distribution (Pre-Augment)\n",
      " Absent     2141\n",
      "Present     366\n",
      "Name: Murmur, dtype: int64\n",
      "\n",
      "Augmenting present class audio data\n",
      "\n",
      "                       filenames patient_id   Murmur Murmur locations\n",
      "16    training_data/14241_AV.wav      14241  Present      AV+MV+PV+TV\n",
      "17    training_data/14241_MV.wav      14241  Present      AV+MV+PV+TV\n",
      "18    training_data/14241_PV.wav      14241  Present      AV+MV+PV+TV\n",
      "19    training_data/14241_TV.wav      14241  Present      AV+MV+PV+TV\n",
      "20    training_data/14998_AV.wav      14998   Absent              NaN\n",
      "...                          ...        ...      ...              ...\n",
      "3158  training_data/85345_AV.wav      85345   Absent              NaN\n",
      "3159  training_data/85345_PV.wav      85345   Absent              NaN\n",
      "3160  training_data/85349_AV.wav      85349   Absent              NaN\n",
      "3161  training_data/85349_PV.wav      85349   Absent              NaN\n",
      "3162  training_data/85349_TV.wav      85349   Absent              NaN\n",
      "\n",
      "[2507 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 366/366 [00:09<00:00, 40.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set class distribution (Post Signal Augment)\n",
      " Absent     2141\n",
      "Present     598\n",
      "Name: Murmur, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# read all audio recordings and augment\n",
    "with open(records,'r') as r:\n",
    "    filenames = r.readlines()\n",
    "\n",
    "    # dataframe containing audio recording path with corresponding patient_id\n",
    "    file_df = pd.DataFrame(filenames,columns=[\"filenames\"])\n",
    "    file_df[\"filenames\"] = file_df[\"filenames\"].str.strip() + '.wav'\n",
    "    file_df[\"patient_id\"] = file_df[\"filenames\"].str.split(\"/\").str[1]\n",
    "    file_df[\"patient_id\"] = file_df[\"patient_id\"].str.split(\"_\").str[0].astype(str)\n",
    "    # merge with demographics to enrich dataframe\n",
    "    demographics[\"Patient ID\"] = demographics[\"Patient ID\"].astype(str)\n",
    "    file_df = file_df.merge(\n",
    "        demographics[[\"Murmur\",\"Patient ID\",\"Murmur locations\"]],\n",
    "        how='left',\n",
    "        left_on='patient_id',\n",
    "        right_on='Patient ID'\n",
    "    )\n",
    "    file_df.pop(\"Patient ID\")\n",
    "# class distribution by audio recordings\n",
    "print(\"Class distribution by audio recordings\"+ \"\\n\" ,file_df[\"Murmur\"].value_counts()) #Heavily biased to absent class\n",
    "\n",
    "# grabbing only present and absent class\n",
    "file_df_present = file_df.loc[file_df[\"Murmur\"]==\"Present\"]\n",
    "file_df_absent = file_df.loc[file_df[\"Murmur\"]==\"Absent\"]\n",
    "\n",
    "# creating test df\n",
    "test_present_ids = grab_id(250,file_df_present[\"patient_id\"].value_counts())\n",
    "test_absent_ids = grab_id(250,file_df_absent[\"patient_id\"].value_counts())\n",
    "test_ids = test_absent_ids+test_present_ids\n",
    "test_df = file_df.loc[file_df[\"patient_id\"].isin(test_ids)]\n",
    "\n",
    "# create train df\n",
    "train_df = file_df.loc[~(file_df[\"patient_id\"].isin(test_ids)) & (file_df[\"Murmur\"] != \"Unknown\")]\n",
    "print(\"\\nTraining set class distribution (Pre-Augment)\\n\",train_df[\"Murmur\"].value_counts())\n",
    "print(\"\\nAugmenting present class audio data\\n\")\n",
    "print(train_df)\n",
    "# augmenting present class audio data\n",
    "filtered_train = train_df.loc[train_df[\"Murmur\"]!=\"Absent\"].copy()\n",
    "for i,k in tqdm(filtered_train.iterrows(),total=filtered_train.shape[0]):\n",
    "    list_of_murmur_locations = list(k[\"Murmur locations\"].split(\"+\"))\n",
    "    ausc_location = k[\"filenames\"].split(\"/\")[1].split(\".\")[0].split(\"_\")[1]\n",
    "    if ausc_location.strip() in list_of_murmur_locations:\n",
    "        S,sr = librosa.load(data_path+k[\"filenames\"],sr=4000)\n",
    "        augmented_signal = augment(S,sr)\n",
    "        augmented_signal_filename = k[\"filenames\"].split(\".\")[0]+\"_a1.wav\"\n",
    "        augment_signal(S,sr,augment_path+augmented_signal_filename)\n",
    "        train_df = train_df.append(pd.DataFrame(\n",
    "            data=[[\n",
    "                augmented_signal_filename,k[\"patient_id\"],k[\"Murmur\"],k[\"Murmur locations\"]\n",
    "                ]],\n",
    "            columns=list(train_df.columns.values)\n",
    "            ))\n",
    "    else:\n",
    "        train_df = train_df.drop(index=train_df.loc[train_df[\"filenames\"]==k[\"filenames\"]].index)\n",
    "train_df.pop(\"Murmur locations\")\n",
    "print(\"\\nTraining set class distribution (Post Signal Augment)\\n\",train_df[\"Murmur\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features and augmenting present class mfcc spectogram\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2739/2739 [04:27<00:00, 10.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set class distribution (Post Spectrogram augment))\n",
      " Absent     2141\n",
      "Present    1196\n",
      "Name: Murmur, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df[\"data\"] = np.array(None)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "# extracting features and augmenting mfcc spectogram for training set\n",
    "print(\"\\nExtracting features and augmenting present class mfcc spectogram\\n\")\n",
    "for i,k in tqdm(train_df.copy().iterrows(),total=train_df.copy().shape[0]):\n",
    "    S,sr = librosa.load(augment_path+k[\"filenames\"],sr=4000)\n",
    "    scaled_mfccs , mfccs = mfccs_features_extract(S,sr)\n",
    "    train_df.loc[i,\"data\"] = scaled_mfccs\n",
    "    if k[\"Murmur\"] != \"Absent\":\n",
    "        mfccs_augment = augment_spec(mfccs)\n",
    "        mfccs_augment = np.mean(mfccs_augment.T,axis=0)\n",
    "        train_df = train_df.append(pd.DataFrame(\n",
    "            data=[[\n",
    "                k[\"filenames\"].replace(\".wav\",\"_augmentspect.wav\"),\n",
    "                k[\"patient_id\"],\n",
    "                k[\"Murmur\"],\n",
    "                mfccs_augment\n",
    "            ]],\n",
    "            columns=list(train_df.columns.values)\n",
    "        ))\n",
    "print(\"\\nTraining set class distribution (Post Spectrogram augment))\\n\",train_df[\"Murmur\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trim absent class data to match size of present class data and shuffle \n",
      "Present    1196\n",
      "Absent     1196\n",
      "Name: Murmur, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# class balancing for training set\n",
    "present_count = train_df[\"Murmur\"].value_counts()[1]\n",
    "print(\"Trim absent class data to match size of present class data and shuffle \")\n",
    "absent_ids = grab_id(present_count,train_df.loc[train_df[\"Murmur\"]==\"Absent\"][\"patient_id\"].value_counts())\n",
    "train_df = pd.concat([\n",
    "    train_df.loc[train_df[\"Murmur\"]==\"Present\"],\n",
    "    train_df.loc[(train_df[\"Murmur\"]==\"Absent\")&(train_df[\"patient_id\"].isin(absent_ids))]\n",
    "]).sample(frac=1)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "print(train_df[\"Murmur\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10391/529885081.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[\"data\"] = None\n",
      "100%|██████████| 500/500 [00:46<00:00, 10.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing set class distribution (final)\n",
      " Absent     250\n",
      "Present    250\n",
      "Name: Murmur, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_df[\"data\"] = None\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "for i,k in tqdm(test_df.copy().iterrows(),total=test_df.copy().shape[0]):\n",
    "    S,sr = librosa.load(data_path+k[\"filenames\"],sr=4000)\n",
    "    test_scaled_mfccs, test_melspect = mfccs_features_extract(S,sr)\n",
    "    test_df.loc[i,\"data\"] = test_scaled_mfccs\n",
    "test_df = test_df.sample(frac=1).reset_index()\n",
    "print(\"\\nTesting set class distribution (final)\\n\",test_df[\"Murmur\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual splitting and assigning data to variables\n",
    "X_test = np.array(test_df[\"data\"].tolist())\n",
    "X_train = np.array(train_df[\"data\"].tolist())\n",
    "y_test = test_df[\"Murmur\"]\n",
    "y_train = train_df[\"Murmur\"]\n",
    "\n",
    "\n",
    "# export data \n",
    "import pickle \n",
    "\n",
    "pickle_out_X_train = open(\"./1.0.1/pickled_data/X_train_autosklearn_v2.pickle\",\"wb\")\n",
    "pickle.dump(X_train,pickle_out_X_train)\n",
    "pickle_out_X_train.close()\n",
    "\n",
    "pickle_out_X_test = open(\"./1.0.1/pickled_data/X_test_autosklearn_v2.pickle\",\"wb\")\n",
    "pickle.dump(X_test,pickle_out_X_test)\n",
    "pickle_out_X_test.close()\n",
    "\n",
    "\n",
    "pickle_out_y_train = open(\"./1.0.1/pickled_data/y_train_autosklearn_v2.pickle\",\"wb\")\n",
    "pickle.dump(y_train,pickle_out_y_train)\n",
    "pickle_out_y_train.close()\n",
    "\n",
    "pickle_out_y_test = open(\"./1.0.1/pickled_data/y_test_autosklearn_v2.pickle\",\"wb\")\n",
    "pickle.dump(y_test,pickle_out_y_test)\n",
    "pickle_out_y_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b43543f65750abaf68843f05bc2f17d6c89e154a5075b0a110863194543eb828"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ubuntu_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
